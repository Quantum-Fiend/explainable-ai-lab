{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# ðŸ§  Elite Explainable AI (XAI) System: Beyond the Black Box\n",
                "\n",
                "## 1. Problem Definition & Advanced Context\n",
                "**Objective**: Predict credit risk with high precision and provide **actionable** explanations.\n",
                "\n",
                "**Production-Ready Features**:\n",
                "- ðŸ¤– **Automated Tuning**: Optuna for optimization.\n",
                "- ðŸ’¾ **Serialization**: Saving the full pipeline for deployment.\n",
                "- ðŸŒ **Counterfactuals**: Actionable user feedback.\n",
                "- ðŸ“Š **Dashboard**: Stakeholder-ready ExplainerDashboard."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ðŸ› ï¸ 1. Setup & Imports\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "import shap\n",
                "import lime\n",
                "import lime.lime_tabular\n",
                "import xgboost as xgb\n",
                "import dice_ml\n",
                "import optuna\n",
                "import joblib  # For serialization\n",
                "from explainerdashboard import ClassifierExplainer, ExplainerDashboard\n",
                "\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.metrics import (accuracy_score, classification_report, \n",
                "                             roc_auc_score, confusion_matrix)\n",
                "from sklearn.pipeline import Pipeline\n",
                "from sklearn.compose import ColumnTransformer\n",
                "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
                "from sklearn.impute import SimpleImputer\n",
                "from sklearn.datasets import fetch_openml\n",
                "\n",
                "# Configuration\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "sns.set_style(\"whitegrid\")\n",
                "plt.rcParams['figure.figsize'] = (12, 6)\n",
                "shap.initjs()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Data Loading & Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load Data\n",
                "data = fetch_openml(data_id=31, as_frame=True, parser='auto')\n",
                "df = data.frame\n",
                "\n",
                "# Map Target: 'bad' -> 1 (Risk), 'good' -> 0 (Safe)\n",
                "target_col = 'class'\n",
                "df['target'] = df[target_col].map({'bad': 1, 'good': 0})\n",
                "df = df.drop(columns=[target_col])\n",
                "\n",
                "# Split Features and Target\n",
                "X = df.drop(columns=['target'])\n",
                "y = df['target']\n",
                "\n",
                "# Identify columns\n",
                "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
                "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
                "\n",
                "# Preprocessing Pipeline\n",
                "numeric_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='median')),\n",
                "    ('scaler', StandardScaler())\n",
                "])\n",
                "\n",
                "categorical_transformer = Pipeline(steps=[\n",
                "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
                "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
                "])\n",
                "\n",
                "preprocessor = ColumnTransformer(\n",
                "    transformers=[\n",
                "        ('num', numeric_transformer, num_cols),\n",
                "        ('cat', categorical_transformer, cat_cols)\n",
                "    ]\n",
                ")\n",
                "\n",
                "# Split\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(\"Data loaded and split successfully.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Advanced Training: Hyperparameter Tuning with Optuna\n",
                "Instead of guessing parameters, we use Bayesian Optimization to find the best XGBoost configuration."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def objective(trial):\n",
                "    param = {\n",
                "        'objective': 'binary:logistic',\n",
                "        'eval_metric': 'logloss',\n",
                "        'use_label_encoder': False,\n",
                "        'n_estimators': trial.suggest_int('n_estimators', 50, 300),\n",
                "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
                "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
                "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
                "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
                "        'random_state': 42\n",
                "    }\n",
                "    \n",
                "    # Create Pipeline with suggested params\n",
                "    model = xgb.XGBClassifier(**param)\n",
                "    clf = Pipeline(steps=[('preprocessor', preprocessor), ('classifier', model)])\n",
                "    \n",
                "    clf.fit(X_train, y_train)\n",
                "    \n",
                "    preds = clf.predict_proba(X_test)[:, 1]\n",
                "    auc = roc_auc_score(y_test, preds)\n",
                "    \n",
                "    return auc\n",
                "\n",
                "print(\"Starting Optuna Optimization...\")\n",
                "study = optuna.create_study(direction='maximize')\n",
                "study.optimize(objective, n_trials=10)  # Low trials for demo speed, increase for production\n",
                "\n",
                "print(\"Best Hyperparameters:\", study.best_params)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Train Final Model with Best Params\n",
                "best_params = study.best_params\n",
                "best_xgb = xgb.XGBClassifier(**best_params, use_label_encoder=False, eval_metric='logloss')\n",
                "\n",
                "final_pipeline = Pipeline(steps=[('preprocessor', preprocessor), \n",
                "                                 ('classifier', best_xgb)])\n",
                "\n",
                "final_pipeline.fit(X_train, y_train)\n",
                "\n",
                "y_pred = final_pipeline.predict(X_test)\n",
                "print(classification_report(y_test, y_pred))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Model Serialization (Production Ready)\n",
                "We save the entire pipeline (including preprocessing) so it can be deployed to a server."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the pipeline\n",
                "model_filename = 'model_pipeline.joblib'\n",
                "joblib.dump(final_pipeline, model_filename)\n",
                "print(f\"Model saved to {model_filename}\")\n",
                "\n",
                "# Verification: Try reloading it\n",
                "loaded_model = joblib.load(model_filename)\n",
                "print(\"Model reloaded successfully. Ready for inference.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Counterfactual Explanations (DiCE)\n",
                "Feature importance tells us *what* matters. Counterfactuals tell us *what to do*.\n",
                "**Scenario**: A customer is rejected. They ask: *\"What do I need to change to be approved?\"*"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# DiCE requires a slightly different data structure\n",
                "# We need to pass the raw (untransformed) data to DiCE, but the model expects transformed data.\n",
                "# So we wrap our pipeline in a class or use DiCE's sklearn backend.\n",
                "\n",
                "import dice_ml\n",
                "\n",
                "# Create DiCE Data object\n",
                "# We use the training data to calculate statistics\n",
                "train_df = X_train.copy()\n",
                "train_df['target'] = y_train\n",
                "\n",
                "d = dice_ml.Data(dataframe=train_df, continuous_features=num_cols, outcome_name='target')\n",
                "\n",
                "# Create DiCE Model object\n",
                "m = dice_ml.Model(model=final_pipeline, backend=\"sklearn\", model_type='classifier')\n",
                "\n",
                "# Initialize DiCE Explainer\n",
                "exp_dice = dice_ml.Dice(d, m, method=\"random\")\n",
                "\n",
                "# Generate counterfactuals for a rejected customer\n",
                "# Pick a customer with prediction = 1 (Bad Risk)\n",
                "risk_indices = np.where(y_test == 1)[0]\n",
                "query_idx = risk_indices[0]\n",
                "query_instance = X_test.iloc[[query_idx]]\n",
                "\n",
                "print(\"Query Instance (High Risk):\")\n",
                "print(query_instance)\n",
                "\n",
                "# Generate CFs\n",
                "dice_exp = exp_dice.generate_counterfactuals(query_instance, total_CFs=3, desired_class=0)\n",
                "\n",
                "# Visualize\n",
                "dice_exp.visualize_as_dataframe(show_only_changes=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Professional Dashboard (ExplainerDashboard)\n",
                "We can launch a full comprehensive dashboard directly within the notebook or on a separate port. This aggregates SHAP, permutation importance, confusion matrix, and individual predictions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# ExplainerDashboard requires a fit model and data\n",
                "# Note: It handles pipelines, but sometimes it's safer to pass the final estimator and transformed data\n",
                "# However, the library supports pipelines well now.\n",
                "\n",
                "print(\"Building ExplainerDashboard... (This may take a moment)\")\n",
                "explainer = ClassifierExplainer(final_pipeline, X_test, y_test)\n",
                "\n",
                "# Start an inline dashboard\n",
                "# You can also run .run(port=8050) to open in a new tab\n",
                "ExplainerDashboard(explainer, mode='inline').run()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Production Inference Simulation\n",
                "This section simulates what happens when the `deployment_test.py` script runs."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def production_predict(input_data):\n",
                "    print(\"Receiving API Request...\")\n",
                "    df_input = pd.DataFrame([input_data])\n",
                "    \n",
                "    # Pipeline handles all preprocessing (Scaling, Encoding)\n",
                "    prob = loaded_model.predict_proba(df_input)[0][1]\n",
                "    print(f\"Risk Probability: {prob:.4f}\")\n",
                "    \n",
                "    return \"REJECT\" if prob > 0.5 else \"APPROVE\"\n",
                "\n",
                "# Mock API Call\n",
                "mock_data = X_test.iloc[0].to_dict()\n",
                "decision = production_predict(mock_data)\n",
                "print(f\"Final Decision: {decision}\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}